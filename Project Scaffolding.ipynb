{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "import glob,os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_distance(from_coord, to_coord):\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(from_coord[1])\n",
    "    lon1 = radians(from_coord[0])\n",
    "    lat2 = radians(to_coord[1])\n",
    "    lon2 = radians(to_coord[0])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    dataframe['distance'] = dataframe.apply(\n",
    "        lambda x: calculate_distance([x['pickup_latitude'],x['pickup_longitude']],\n",
    "                                     [x['dropoff_latitude'],x['dropoff_longitude']]),\n",
    "        axis =1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pattern = r\"yellow_tripdata_201([0-4]{1})|yellow_tripdata_2009|yellow_tripdata_2015-0([1-6]{1})\"\n",
    "\n",
    "def get_taxi_html():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    return html\n",
    "\n",
    "def find_taxi_csv_urls():\n",
    "    soup = bs4.BeautifulSoup(get_taxi_html(), \"html.parser\")\n",
    "    link_list = [a['href'] for a in soup.find_all('a')]\n",
    "    new_link = []\n",
    "    for item in link_list:\n",
    "        match = re.findall(pattern,item)\n",
    "        if len(match)>0 :\n",
    "            new_link.append(item)\n",
    "    return new_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#create the dataframe that converting pickup ID to lat/lon\n",
    "taxi_zones = gpd.read_file(\"/Users/yzh/Documents/GitHub/taxi-cab-project/taxi_zones.shp\")\n",
    "taxi_zones = taxi_zones.to_crs(4326)\n",
    "taxi_zones['Lon'] = taxi_zones.centroid.x\n",
    "taxi_zones['Lat'] = taxi_zones.centroid.y\n",
    "taxi_zones.drop(columns = \"geometry\", axis = 1,inplace = True)\n",
    "taxi_zones = taxi_zones[['LocationID', 'Lon', 'Lat']]\n",
    "taxi_zones_PU = taxi_zones\n",
    "taxi_zones_PU.columns = ['PULocationID','pickup_longitude','pickup_latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataframe that converting dropoff ID to lat/lon\n",
    "taxi_zones2 = gpd.read_file(\"/Users/yzh/Documents/GitHub/taxi-cab-project/taxi_zones.shp\")\n",
    "taxi_zones2 = taxi_zones2.to_crs(4326)\n",
    "taxi_zones2['Lon'] = taxi_zones2.centroid.x\n",
    "taxi_zones2['Lat'] = taxi_zones2.centroid.y\n",
    "taxi_zones2.drop(columns = \"geometry\", axis = 1,inplace = True)\n",
    "taxi_zones2 = taxi_zones2[['LocationID', 'Lon', 'Lat']]\n",
    "taxi_zones_DO = taxi_zones2\n",
    "taxi_zones_DO.columns = ['DOLocationID','dropoff_longitude','dropoff_latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long(df):\n",
    "    idx_start = df.index[(df['pickup_latitude'] < 40.560445) | (df['pickup_longitude'] < -74.242330)].tolist()\n",
    "    idx_end   = df.index[(df['dropoff_latitude'] > 40.908524) | (df['dropoff_longitude'] > -73.717047)].tolist()\n",
    "\n",
    "  # Remove duplicate elemets\n",
    "    idx_final = set(idx_start + idx_end)\n",
    "\n",
    "  # Drop elements\n",
    "    df.drop(idx_final, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_and_clean_month_taxi_data(url):\n",
    "    data = requests.get(url).content\n",
    "    name = link.rsplit(\"/\",1)[-1]\n",
    "    with open(name, 'wb') as f:\n",
    "        f.write(data)\n",
    "    df = pd.read_parquet(name)\n",
    "    df = df.sample(frac=0.0002, replace=True, random_state=1)\n",
    "    \n",
    "    if \"pickup_datetime\" in df:\n",
    "        #normalize the column name\n",
    "        df.rename(columns = {\"tip_amount\":\"Tip_Amt\"}, inplace = True)\n",
    "        \n",
    "        #select the columns we need\n",
    "        df=df[['pickup_datetime','dropoff_datetime','fare_amount','pickup_longitude','pickup_latitude','dropoff_longitude',\n",
    "               'dropoff_latitude','passenger_count','Tip_Amt']]\n",
    "        df = df[df['passenger_count'] != 0]\n",
    "\n",
    "        #filter trips with no fare\n",
    "        df = df[df['fare_amount'] != 0]\n",
    "\n",
    "        #filter trips with no distance between pickup and dropoff points\n",
    "        df = df[(df['pickup_longitude'] != df['dropoff_longitude']) | \n",
    "                (df['pickup_longitude'] != df['dropoff_longitude'])]\n",
    "\n",
    "        #remove unnecessary columns\n",
    "        df=lat_long(df)\n",
    "            \n",
    "        #convert pickup and dropoff times to right type\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "        return df\n",
    "        \n",
    "    #the second pattern\n",
    "    if \"Start_Lon\" in df:\n",
    "        df.rename(columns = {'Start_Lon':'pickup_longitude', 'Start_Lat':'pickup_latitude','End_Lon':'dropoff_longitude',\n",
    "                            'End_Lat':'dropoff_latitude',\"Trip_Pickup_DateTime\":\"pickup_datetime\",\n",
    "                            'Trip_Dropoff_DateTime':'dropoff_datetime','Passenger_Count':'passenger_count',\n",
    "                            'Fare_Amt':'fare_amount'}, inplace = True)\n",
    "        #keeping columns we want\n",
    "        df=df[['pickup_datetime','dropoff_datetime','fare_amount','pickup_longitude','pickup_latitude','dropoff_longitude',\n",
    "                        'dropoff_latitude','passenger_count','Tip_Amt']]\n",
    "           \n",
    "        #filter trips with zero passenger_count\n",
    "        df = df[df['passenger_count'] != 0]\n",
    "\n",
    "        #filter trips with no fare\n",
    "        df = df[df['fare_amount'] != 0]\n",
    "\n",
    "        #filter trips with no distance between pickup and dropoff points\n",
    "        df = df[(df['pickup_longitude'] != df['dropoff_longitude']) | \n",
    "                    (df['pickup_longitude'] != df['dropoff_longitude'])]\n",
    "\n",
    "        #remove unnecessary columns\n",
    "        df=lat_long(df)\n",
    "            \n",
    "        #convert pickup and dropoff times to right type\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "        return df\n",
    "\n",
    "    #third pattern\n",
    "    #first we convert locationid to the latitudes and longitudes\n",
    "    if \"PULocationID\" in df:\n",
    "        df = pd.merge(df, taxi_zones_PU, how='left', on=['PULocationID'])\n",
    "        df = pd.merge(df, taxi_zones_DO, how='left', on=['DOLocationID'])           \n",
    "        \n",
    "        #normalize the column name\n",
    "        df.rename(columns = {\"tip_amount\":\"Tip_Amt\",\"tpep_pickup_datetime\":\"pickup_datetime\", 'tpep_dropoff_datetime':'dropoff_datetime'}, inplace = True)\n",
    "        \n",
    "        #keeping columns we want\n",
    "        df=df[['pickup_datetime','dropoff_datetime','fare_amount','pickup_longitude','pickup_latitude','dropoff_longitude',\n",
    "                        'dropoff_latitude','passenger_count','Tip_Amt']]\n",
    "        #filter trips with zero passenger_count\n",
    "        df = df[df['passenger_count'] != 0]\n",
    "\n",
    "        #filter trips with no fare\n",
    "        df = df[df['fare_amount'] != 0]\n",
    "\n",
    "        #filter trips with no distance between pickup and dropoff points\n",
    "        df = df[(df['pickup_longitude'] != df['dropoff_longitude']) | \n",
    "                (df['pickup_longitude'] != df['dropoff_longitude'])]\n",
    "\n",
    "        #remove trips not in coordinate box by calling functions\n",
    "        df=lat_long(df)\n",
    "            \n",
    "        #convert pickup and dropoff times to right type\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afb116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "We are calling the uber data and cleaning it (removing missing data/NA, dropping coordinates outside our box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \n",
    "    #load csv file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #filter longitude\n",
    "    df = df[(df['pickup_longitude'] <= -73.717047) & (df['dropoff_longitude'] <= -73.717047) & \n",
    "            (df['pickup_longitude'] >= -74.242330) & (df['dropoff_longitude'] >= -74.242330)]\n",
    "    \n",
    "    #filter latitude\n",
    "    df = df[(df['pickup_latitude'] <= 40.908524) & (df['dropoff_latitude'] <= 40.908524) & \n",
    "            (df['pickup_latitude'] >= 40.560445) & (df['dropoff_latitude'] >= 40.560445)]\n",
    "    \n",
    "    #filter trips with zero passenger_count\n",
    "    df = df[df['passenger_count'] != 0]\n",
    "    \n",
    "    #filter trips with no fare\n",
    "    df = df[df['fare_amount'] != 0]\n",
    "    \n",
    "    #filter trips with no distance between pickup and dropoff points\n",
    "    df = df[(df['pickup_longitude'] != df['dropoff_longitude']) | \n",
    "            (df['pickup_longitude'] != df['dropoff_longitude'])]\n",
    "    \n",
    "    #remove unnecessary columns\n",
    "    df = df.drop(['Unnamed: 0','key'], axis=1)\n",
    "\n",
    "    #change pickup_datetime from object type to datetime time\n",
    "    df['pickup_datetime'] = df['pickup_datetime'].str.split(' ', n=2,expand=True)[1]\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "UBER_DATA = \"Downloads/uber_rides_sample.csv\"\n",
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "We are calling the weather data and cleaning it (filling in for missing values, dropping unecessary columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    #form the hourly weather data\n",
    "    df1 = csv_file[['DATE','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "    \n",
    "    #change DATE from object type to datetime type\n",
    "    df1['DATE'] = pd.to_datetime(df1['DATE'])\n",
    "    \n",
    "    #drop na rows\n",
    "    df1 = df1[df1[['HourlyPrecipitation','HourlyWindSpeed']].notnull().any(axis=1)]\n",
    "    \n",
    "    return df1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    #form the daily weather data\n",
    "    df2 = csv_file[['DATE','HourlyWindSpeed','DailyAverageWindSpeed', 'DailyPrecipitation', 'DailySustainedWindSpeed']]\n",
    "    \n",
    "    #change DATE from object type to datetime type\n",
    "    df2['DATE'] = pd.to_datetime(df2['DATE'])\n",
    "    \n",
    "    #Get missing DailyAverageWindSpeed on 2014-01-26 by calculating the mean HourlyWindSpeed on 2014-01-26\n",
    "    missing_day_df = df2[(df2['DATE'] > '2014-01-26') & (df2['DATE'] < '2014-01-27')]\n",
    "    mean = missing_day_df.HourlyWindSpeed.mean()\n",
    "    \n",
    "    #drop na rows\n",
    "    df2 = df2[df2[['DailyPrecipitation', 'DailyAverageWindSpeed', 'DailySustainedWindSpeed']].notnull().any(axis=1)]\n",
    "    df2 = df2[['DATE','DailyPrecipitation', 'DailyAverageWindSpeed', 'DailySustainedWindSpeed']]\n",
    "    \n",
    "    #Update the missing value\n",
    "    df2[\"DailyAverageWindSpeed\"].fillna(mean,inplace=True)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    path = r'Desktop/weather'\n",
    "    file = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    print(file)\n",
    "    weather_csv_files = []\n",
    "    for f in file:\n",
    "        weather_csv_files.append(pd.read_csv(f))\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "we compile the individual files into bigger dataframes for each data type (taxi, uber, weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_data, daily_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "We know clean the data and store it in SQL databases. We will create four tables (taxi data, uber data, hourly weather data and daily weather data) with the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "   id INTEGER PRIMARY KEY,\n",
    "   date DATE,\n",
    "   hourly_precipitation FLOAT,\n",
    "   hourly_wind_speed FLOAT\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "   id INTEGER PRIMARY KEY,\n",
    "   date DATE,\n",
    "   daily_precipitation FLOAT,\n",
    "   daily_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "   id INTEGER PRIMARY KEY,\n",
    "   fare_amount FLOAT,\n",
    "   pickup_datetime DATE,\n",
    "   dropoff_datatime DATE,\n",
    "   pickup_longitude FLOAT,\n",
    "   pickup_latitude FLOAT,\n",
    "   dropoff_longitude FLOAT,\n",
    "   dropoff_latitude FLOAT,\n",
    "   passenger_count INTEGER,\n",
    "   Tip_Amout FLOAT,\n",
    "   distance FLOAT\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "   id INTEGER PRIMARY KEY,\n",
    "   fare_amount FLOAT,\n",
    "   pickup_datetime DATE,\n",
    "   pickup_longitude FLOAT,\n",
    "   pickup_latitude FLOAT,\n",
    "   dropoff_longitude FLOAT,\n",
    "   dropoff_latitude FLOAT,\n",
    "   passenger_count INTEGER,\n",
    "   distance FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "We know add our dataframes to the tables we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for key, value in table_to_df_dict.items():\n",
    "        value.to_sql(key, con=engine, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "We know create queries that will select the columns and data needed to answer questions and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT hour, COUNT(*) as num_trips\n",
    "FROM (\n",
    "  SELECT strftime('%H', pickup_datetime) as hour\n",
    "  FROM taxi_trips\n",
    ")\n",
    "GROUP BY hour\n",
    "ORDER BY num_trips DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT wkday, COUNT(*) as num_trips\n",
    "FROM (\n",
    "  SELECT strftime('%w', pickup_datetime) as wkday\n",
    "  FROM uber_trips\n",
    ")\n",
    "GROUP BY wkday\n",
    "ORDER BY num_trips DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 =  \"\"\"\n",
    "SELECT distance FROM(\n",
    "SELECT distance FROM taxi_trips\n",
    "UNION\n",
    "SELECT distance FROM uber_trips\n",
    ")\n",
    "LIMIT 1\n",
    "OFFSET(\n",
    "SELECT COUNT(*) FROM (\n",
    "SELECT distance FROM taxi_trips\n",
    "UNION\n",
    "SELECT distance FROM uber_trips\n",
    ")\n",
    ") * 95/100 - 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\" \n",
    "SELECT strftime('%Y-%m-%d', pickup_datetime), AVG(distance) as avg_distance\n",
    "FROM (\n",
    "  SELECT pickup_datetime, distance FROM taxi_trips \n",
    "  WHERE strftime('%Y', pickup_datetime) = '2009' \n",
    "  UNION ALL\n",
    "  SELECT pickup_datetime, distance FROM uber_trips \n",
    "  WHERE strftime('%Y', pickup_datetime) = '2009'\n",
    ")\n",
    "GROUP BY strftime('%Y-%m-%d', pickup_datetime)\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e01db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\" \n",
    "SELECT strftime('%Y-%m-%d', pickup_datetime), COUNT(*) as num_trips\n",
    "FROM (\n",
    "  SELECT pickup_datetime FROM taxi_trips \n",
    "  WHERE strftime('%Y', pickup_datetime) = '2014' \n",
    "  UNION ALL\n",
    "  SELECT pickup_datetime FROM uber_trips \n",
    "  WHERE strftime('%Y', pickup_datetime) = '2014'\n",
    ")\n",
    "WHERE strftime('%Y-%m-%d', pickup_datetime) IN (\n",
    "  SELECT strftime('%Y-%m-%d', DATE) FROM daily_weather\n",
    "  WHERE strftime('%Y', DATE) = '2014'\n",
    "  ORDER BY DailyAverageWindSpeed DESC\n",
    ")\n",
    "GROUP BY strftime('%Y-%m-%d', pickup_datetime)\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13019353",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\" \n",
    "SELECT strftime('%Y-%m-%d %H', pickup_datetime) as hourtime, COUNT(*) as num_trips\n",
    "FROM (\n",
    "  SELECT pickup_datetime FROM taxi_trips \n",
    "  WHERE taxi_trips.pickup_datetime BETWEEN '2012-10-22' AND '2012-11-05' \n",
    "  UNION ALL\n",
    "  SELECT pickup_datetime FROM uber_trips \n",
    "  WHERE uber_trips.pickup_datetime BETWEEN '2012-10-22' AND '2012-11-05'\n",
    ")\n",
    "GROUP BY hourtime\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()\n",
    "engine.execute(QUERY_2).fetchall()\n",
    "engine.execute(QUERY_3).fetchall()\n",
    "engine.execute(QUERY_4).fetchall()\n",
    "engine.execute(QUERY_5).fetchall()\n",
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"Most_popular_hour_yellow_taxi_200901_201506.sql\")\n",
    "write_query_to_file(QUERY_2, \"Most_popular_weekday_uber_200901_201506.sql\")\n",
    "write_query_to_file(QUERY_3, \"95_percentile_distance_all_201307.sql\")\n",
    "write_query_to_file(QUERY_4, \"Top10_highest_hired_day_with_avg_distance_2009.sql\")\n",
    "write_query_to_file(QUERY_5, \"Top10_windiest_day_with_hired_trips_count_2014.sql\")\n",
    "write_query_to_file(QUERY_6, \"Hurricane_Sandy_data.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "We know create different graphics that showcase the data. The data used for the graphs is called from our SQL tables. Simply just call each function to display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hours_taxi_trips():\n",
    "\n",
    "    # Get the results from the query and store them in two lists\n",
    "    hour, number_trips = [], []\n",
    "    \n",
    "    for row in engine.execute(QUERY_1 ).fetchall():\n",
    "        hour.append(row[0])\n",
    "        number_trips.append(row[1])\n",
    "\n",
    "# Use matplotlib to create a bar chart showing the number of trips per hour\n",
    "        plt.bar(hour, number_trips)\n",
    "        plt.xlabel('Hour of the Day')\n",
    "        plt.ylabel('Number of Taxi Trips')\n",
    "        plt.title(\"Number of taxi trips taken for each hour of the day\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ede9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_taxi_trips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_distance_month():\n",
    "# get the average distance traveled per month for taxis and Ubers combined\n",
    "    query_7 = \"\"\"\n",
    "        SELECT strftime('%m', pickup_datetime) as month, AVG(distance) as avg_distance\n",
    "        FROM uber_trips,taxi_trips\n",
    "        GROUP BY month\n",
    "        ORDER BY month;\n",
    "        \"\"\"\n",
    "\n",
    "    # Get the results from the query and store them in two lists\n",
    "    months, avg_distances = [], []\n",
    "    for row in engine.execute(query_7).fetchall():\n",
    "        months.append(row[0])\n",
    "        avg_distances.append(row[1])\n",
    "\n",
    "    # Calculate the standard deviation and the 90% confidence interval for the average distance traveled per month\n",
    "    stdev = np.std(avg_distances)\n",
    "    confidence_interval = 1.645 * stdev\n",
    "    upper_bound = [avg + confidence_interval for avg in avg_distances]\n",
    "    lower_bound = [avg - confidence_interval for avg in avg_distances]\n",
    "\n",
    "    # Use matplotlib to create a bar chart showing the average distance traveled per month,\n",
    "    # with the 90% confidence interval around the mean\n",
    "    plt.bar(months, avg_distances, yerr=[upper_bound, lower_bound])\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Distance Traveled (km)')\n",
    "    plt.title('Average distance traveled per month')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53960e47",
   "metadata": {},
   "source": [
    "avg_distance_month()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_distance():\n",
    "    # retrieve the tip amount and distance for each Yellow Taxi ride\n",
    "    query_08 = \"\"\"SELECT tip_amount, distance FROM taxi_trips;\"\"\"\n",
    "    results = engine.execute(query_08).fetchall()\n",
    "    tip, distance = [], []\n",
    "    for row in engine.execute(query_08).fetchall():\n",
    "        tip.append(row[0])\n",
    "        distance.append(row[1])\n",
    "\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.scatter(tip, distance)\n",
    "    plt.xlabel(\"Tip amount\")\n",
    "    plt.ylabel(\"Distance (km)\")\n",
    "    plt.title(\"Tip amount vs. Distance for Taxi rides\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16effd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5232312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_rain():\n",
    "    # retrieve the tip amount and distance for each Yellow Taxi ride\n",
    "    query = \"\"\"SELECT tip_amount, hourly_precipitation FROM taxi_trips,hourly_weather;\"\"\"\n",
    "    results = engine.execute(query).fetchall()\n",
    "    tip, precipitation = [], []\n",
    "    for row in engine.execute(query).fetchall():\n",
    "        tip.append(row[0])\n",
    "        precipitation.append(row[1])\n",
    "\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.scatter(tip, precipitation)\n",
    "    plt.xlabel(\"Tip amount\")\n",
    "    plt.ylabel(\"Precipitation\")\n",
    "    plt.title(\"Tip amount vs. Precipitation for Taxi rides\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b198d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_rain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaca2ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d54b48331d607517627292df9e2160ac1e5df3fba116c8c1dd156b3b71ccf6a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
